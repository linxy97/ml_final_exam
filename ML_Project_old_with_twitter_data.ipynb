{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edfcIbFJOSn6"
   },
   "source": [
    "#Part I: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A5tgf8B2F3NH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Madhu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INfbDvCly8L5",
    "outputId": "92364bf0-9920-49bd-c090-7dc8d7eb7a7b"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjtBdgK5Ocny"
   },
   "source": [
    "#Part 2: Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-ZW4vwBPGbzt",
    "outputId": "873220b5-d731-47aa-96d9-52a99d25ae68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query         username  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('sentiment140.csv', encoding = \"ISO-8859-1\", header=None)\n",
    "df_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmI8cE6RL7Cv",
    "outputId": "5da560d6-fb16-4c84-a5e7-2f229fad1ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    800000\n",
      "0    800000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the data's output balance\n",
    "# The label '4' denotes positive sentiment and '0' denotes negative sentiment\n",
    "print(df_raw['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC3D2NGPL8A2",
    "outputId": "2eb5fe9b-0d92-4cfb-c5c0-95b1963c35bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We then change all the values of 4 to 1\n",
    "df_raw['label'] = (df_raw['label'] == 4).astype(int)\n",
    "df_raw['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1xzIShCNqc3",
    "outputId": "6d57fc47-9bd2-4e49-bcfd-430975dd3ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 800000\n"
     ]
    }
   ],
   "source": [
    "# Ommiting every column except for the text and the label, as we won't need any of the other information\n",
    "df = df_raw[['label', 'text']]\n",
    "df.head()\n",
    "# Seperating positive and negative rows\n",
    "df_pos = df[df['label'] == 1]\n",
    "df_neg = df[df['label'] == 0]\n",
    "print(len(df_pos), len(df_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rADp9tN_OobY"
   },
   "source": [
    "##Print examples of positive and negative tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqMfYgHTOtd7",
    "outputId": "3810b2ad-d628-41b7-8698-e614d3c78a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE:\n",
      "\n",
      "0:  I LOVE @Health4UandPets u guys r the best!! \n",
      "1:  im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\n",
      "2:  @DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. \n",
      "3:  Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup\n",
      "4:  @LovesBrooklyn2 he has that effect on everyone \n"
     ]
    }
   ],
   "source": [
    "print('POSITIVE:\\n')\n",
    "for i in range(5):\n",
    "    print(str(i) + ':  ' + df_pos['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-ijJgAkQd32",
    "outputId": "990656e7-32a2-446a-dd44-7ba386b8f8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE:\n",
      "\n",
      "0:  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "1:  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "2:  @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "3:  my whole body feels itchy and like its on fire \n",
      "4:  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n"
     ]
    }
   ],
   "source": [
    "print('NEGATIVE:\\n')\n",
    "for i in range(5):\n",
    "    print(str(i) + ':  ' + df_neg['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "rnsgGCeu3goZ",
    "outputId": "3158422d-0fa2-4913-bd9f-e5597d113b6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602995</th>\n",
       "      <td>0</td>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602996</th>\n",
       "      <td>0</td>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602997</th>\n",
       "      <td>0</td>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602998</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602999</th>\n",
       "      <td>0</td>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1603000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1           0  is upset that he can't update his Facebook by ...\n",
       "2           0  @Kenichan I dived many times for the ball. Man...\n",
       "3           0    my whole body feels itchy and like its on fire \n",
       "4           0  @nationwideclass no, it's not behaving at all....\n",
       "...       ...                                                ...\n",
       "1602995     0  The screen does get smudged easily because it ...\n",
       "1602996     0  What a piece of junk.. I lose more calls on th...\n",
       "1602997     0                       Item Does Not Match Picture.\n",
       "1602998     0  The only thing that disappoint me is the infra...\n",
       "1602999     0  You can not answer calls with the unit, never ...\n",
       "\n",
       "[1603000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add UCI dataset to sentiment140 dataset\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "f = open('imdb_labelled.txt', 'r')\n",
    "for line in f:\n",
    "    if line.find('  \\t') != -1:\n",
    "        label.append(line[-2])\n",
    "        index = line.find('  \\t')\n",
    "        text.append(line[:index])\n",
    "    elif line.find('   ') != -1:\n",
    "        label.append(line[-2])\n",
    "        index = line.find('   ')\n",
    "        text.append(line[:index])\n",
    "f.close()\n",
    "\n",
    "f = open('yelp_labelled.txt', 'r')\n",
    "for line in f:\n",
    "    if line.find('\\t') != -1:\n",
    "        label.append(line[-2])\n",
    "        index = line.find('\\t')\n",
    "        text.append(line[:index])\n",
    "f.close()\n",
    "\n",
    "f = open('amazon_cells_labelled.txt', 'r')\n",
    "for line in f:\n",
    "    if line.find('\\t') != -1:\n",
    "        label.append(line[-2])\n",
    "        index = line.find('\\t')\n",
    "        text.append(line[:index])\n",
    "f.close()\n",
    "\n",
    "df_UCI = pd.DataFrame({\"label\":label, \"text\":text})\n",
    "df.append(df_UCI, ignore_index = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUUo6h1TQlVx"
   },
   "source": [
    "#Part 3: Data Cleaning and Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQH9hMLNQ0vs"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGQn5jYIRCV-"
   },
   "source": [
    "To clean the data, we will first use nltk.tokenize.casual module, which is a twitter-aware tokenizer, designed to be flexible and easy to adapt to new domains and tasks. The basic logic is this:\n",
    "1. The tuple regex_strings defines a list of regular expression strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the user-supplied string, inside the tokenize() method of the class Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option: preserve_case. By default, it is set to True. If it is set to False, then the tokenizer will downcase everything except for emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-_A0-HERBxa",
    "outputId": "04b22860-f184-47b1-f3ad-339e9b275440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 101.72077703475952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['@switchfoot',\n",
       "   'http://twitpic.com/2y1zl',\n",
       "   '-',\n",
       "   'Awww',\n",
       "   ',',\n",
       "   \"that's\",\n",
       "   'a',\n",
       "   'bummer',\n",
       "   '.',\n",
       "   'You',\n",
       "   'shoulda',\n",
       "   'got',\n",
       "   'David',\n",
       "   'Carr',\n",
       "   'of',\n",
       "   'Third',\n",
       "   'Day',\n",
       "   'to',\n",
       "   'do',\n",
       "   'it',\n",
       "   '.',\n",
       "   ';D'],\n",
       "  0),\n",
       " (['is',\n",
       "   'upset',\n",
       "   'that',\n",
       "   'he',\n",
       "   \"can't\",\n",
       "   'update',\n",
       "   'his',\n",
       "   'Facebook',\n",
       "   'by',\n",
       "   'texting',\n",
       "   'it',\n",
       "   '...',\n",
       "   'and',\n",
       "   'might',\n",
       "   'cry',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   'School',\n",
       "   'today',\n",
       "   'also',\n",
       "   '.',\n",
       "   'Blah',\n",
       "   '!'],\n",
       "  0),\n",
       " (['@Kenichan',\n",
       "   'I',\n",
       "   'dived',\n",
       "   'many',\n",
       "   'times',\n",
       "   'for',\n",
       "   'the',\n",
       "   'ball',\n",
       "   '.',\n",
       "   'Managed',\n",
       "   'to',\n",
       "   'save',\n",
       "   '50',\n",
       "   '%',\n",
       "   'The',\n",
       "   'rest',\n",
       "   'go',\n",
       "   'out',\n",
       "   'of',\n",
       "   'bounds'],\n",
       "  0),\n",
       " (['my',\n",
       "   'whole',\n",
       "   'body',\n",
       "   'feels',\n",
       "   'itchy',\n",
       "   'and',\n",
       "   'like',\n",
       "   'its',\n",
       "   'on',\n",
       "   'fire'],\n",
       "  0),\n",
       " (['@nationwideclass',\n",
       "   'no',\n",
       "   ',',\n",
       "   \"it's\",\n",
       "   'not',\n",
       "   'behaving',\n",
       "   'at',\n",
       "   'all',\n",
       "   '.',\n",
       "   \"i'm\",\n",
       "   'mad',\n",
       "   '.',\n",
       "   'why',\n",
       "   'am',\n",
       "   'i',\n",
       "   'here',\n",
       "   '?',\n",
       "   'because',\n",
       "   'I',\n",
       "   \"can't\",\n",
       "   'see',\n",
       "   'you',\n",
       "   'all',\n",
       "   'over',\n",
       "   'there',\n",
       "   '.'],\n",
       "  0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import random\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "start_time = time()\n",
    "# The redule_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n",
    "# For example, it will transform the word: 'Helloooooooooo' to: 'Hellooo'\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df['text'].tolist()\n",
    "Y = df['label'].tolist()\n",
    "\n",
    "#Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text and its corresponding label\n",
    "for x, y in zip(X, Y):\n",
    "    if y == 1:\n",
    "        data.append((tk.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((tk.tokenize(x), 0))\n",
    "        \n",
    "# Printing the CPU time and the first 5 elements of our 'data' list\n",
    "print('CPU Time:', time() - start_time)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJvDJEPrVwl6"
   },
   "source": [
    "##Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxQYJ7czV8kG"
   },
   "source": [
    "We will use nltk's WordNetLemmatizer to lemmatize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dchIM9Tp28ET"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def clean_data(data):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, label in pos_tag(data):\n",
    "        #remove links\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # remove @\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        #lemmatize\n",
    "        if label.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif label.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "    \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        cleaned_token = token.lower()\n",
    "        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgR_kvzv6jGe"
   },
   "source": [
    "# Baseline Method 1: Bayesian Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9GIvShV6pv0",
    "outputId": "e7d574f2-b316-4ddc-81f7-41660dc6057b"
   },
   "outputs": [],
   "source": [
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "for tokens, label in data:\n",
    "    cleaned_tokens_list.append((clean_data(tokens), label))\n",
    "\n",
    "print('Cleaned Data, CPU Time:', time() - start_time)\n",
    "start_time = time()\n",
    "final_data = []\n",
    "for tokens, label in cleaned_tokens_list:\n",
    "    final_data.append((list_to_dict(tokens), label))\n",
    "\n",
    "final_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImxRwEgLYm4y",
    "outputId": "1d84cafa-61bc-4d94-8b40-3275e1bdb6d2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "random.Random(140).shuffle(final_data)\n",
    "\n",
    "trim_index = int(len(final_data) * 0.9)\n",
    "\n",
    "train_data = final_data[:trim_index]\n",
    "test_data = final_data[trim_index:]\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Output the model accuracy on the train and test data\n",
    "print('Accuracy on train data:', classify.accuracy(classifier, train_data))\n",
    "print('Accuracy on test data:', classify.accuracy(classifier, test_data))\n",
    "\n",
    "# Output the words that provide the most information about the sentiment of a tweet.\n",
    "# These are words that are heavily present in one sentiment group and very rarely present in the other group.\n",
    "print(classifier.show_most_informative_features(25))\n",
    "\n",
    "print('\\nCPU Time:', time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8UF3FnlaD6Z",
    "outputId": "8963edff-a6bb-478b-d6f5-fb381ea223d0"
   },
   "outputs": [],
   "source": [
    "custom_tweet = \"bad\"\n",
    "\n",
    "custom_tokens = clean_data(tk.tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RCFYliExmEk"
   },
   "source": [
    "# SVM using TF-IDF library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "1NB6hxAtxmEk",
    "outputId": "d994da13-afb9-4db8-cb29-6c396ed34250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "a = []\n",
    "b = []\n",
    "for i in range(1000):\n",
    "    a.append(X[i])\n",
    "for i in range(1000):\n",
    "    a.append(X[len(X) - 1 - i])\n",
    "for i in range(1000):\n",
    "    b.append(y[i])\n",
    "for i in range(1000):\n",
    "    b.append(y[len(y) - 1 - i])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.20)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(svclassifier.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjIXXNvPxmEk"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Implementation of SVM with Kernel Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from functools import wraps\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cache_decorator():\n",
    "    \"\"\"\n",
    "    Cache decorator. Stores elements to avoid repeated computations.\n",
    "    For more details see: https://stackoverflow.com/questions/36684319/decorator-for-a-class-method-that-caches-return-value-after-first-access\n",
    "    \"\"\"\n",
    "    def wrapper(function):\n",
    "        \"\"\"\n",
    "        Return element if in cache. Otherwise compute and store.\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "\n",
    "        @wraps(function)\n",
    "        def element(*args):\n",
    "            if args in cache:\n",
    "                result = cache[args]\n",
    "            else:\n",
    "                result = function(*args)\n",
    "                cache[args] = result\n",
    "            return result\n",
    "\n",
    "        def clear():\n",
    "            \"\"\"\n",
    "            Clear cache.\n",
    "            \"\"\"\n",
    "            cache.clear()\n",
    "\n",
    "        # Clear the cache\n",
    "        element.clear = clear\n",
    "        return element\n",
    "    return wrapper\n",
    "\n",
    "class Kernel(object):\n",
    "    def evaluate(self, s, t):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_kernel_matrix(self, *, X, X_prime=None):\n",
    "        X_prime = X if not X_prime else X_prime\n",
    "        kernel_matrix = np.zeros((len(X), len(X_prime)), dtype=np.float32)\n",
    "\n",
    "        for row in range(len(X)):\n",
    "            for col in range(len(X_prime)):\n",
    "                kernel_matrix[row][col] = self.evaluate(X[row],X_prime[col])\n",
    "        return kernel_matrix\n",
    "\n",
    "\n",
    "class NgramKernel(Kernel):\n",
    "    def __init__(self, *, ngram_length):\n",
    "        self.ngram_length = ngram_length\n",
    "\n",
    "\n",
    "    def generate_ngrams(self, doc):\n",
    "        ngrams = set()\n",
    "        for i in range(len(doc) - self.ngram_length + 1):\n",
    "            ngrams.add(doc[i : i + self.ngram_length])\n",
    "        return ngrams\n",
    "    \n",
    "    @cache_decorator()\n",
    "    def evaluate(self, s, t):\n",
    "        s_ngrams = self.generate_ngrams(s)\n",
    "        t_ngrams = self.generate_ngrams(t)\n",
    "        x = len(s_ngrams.intersection(t_ngrams))\n",
    "        y = len(s_ngrams.union(t_ngrams))\n",
    "        if y == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return x / y\n",
    "\n",
    "\n",
    "class TFIDFKernel(Kernel):\n",
    "    def __init__(self, *, X, X_prime=None):\n",
    "        self.tfidf = self.compute_tfidf(X, X_prime)\n",
    "        \n",
    "\n",
    "    def compute_tf(self, doc):\n",
    "        words = doc.split()\n",
    "        tf = Counter(words)\n",
    "        for key in tf.keys():\n",
    "            tf[key] = tf[key]/len(words)\n",
    "        return tf\n",
    "\n",
    "    def compute_df(self, X, vocab):\n",
    "        df = defaultdict(int)\n",
    "        for word in vocab:\n",
    "            num_docs = 0\n",
    "            for doc in X:\n",
    "                if word in doc:\n",
    "                    num_docs += 1\n",
    "            df[word] = num_docs\n",
    "        return df\n",
    "\n",
    "\n",
    "    def compute_tfidf(self, X, X_prime):\n",
    "        if X_prime:\n",
    "            X = X + X_prime\n",
    "\n",
    "        tf_idf = defaultdict(float)\n",
    "        words = []\n",
    "        N = len(X)\n",
    "\n",
    "        for doc in X:\n",
    "            words.extend(doc.split())\n",
    "\n",
    "        vocab = set(words)\n",
    "        df = self.compute_df(X, vocab)\n",
    "\n",
    "        for doc in X:\n",
    "            tf = self.compute_tf(doc)\n",
    "            for word in tf.keys():\n",
    "                tf_idf[(doc, word)] = tf[word] * np.log(len(X) / (df[word] + 1))\n",
    "\n",
    "        return tf_idf\n",
    "\n",
    "    @cache_decorator()\n",
    "    def evaluate(self, s, t):\n",
    "        k = 0.0\n",
    "        \n",
    "        words = s.split()\n",
    "        s_unique_words = set(words)\n",
    "        t_words = t.split()\n",
    "        tf = self.compute_tf(t)\n",
    "\n",
    "        for word in s_unique_words:\n",
    "            if word in t_words:\n",
    "                freq = tf[word]\n",
    "                k += freq * self.tfidf[(s, word)]\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, lmbda):\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def fit(self, *, X, y, kernel_matrix):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, X, kernel_matrix):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class KernelPegasos(Model):\n",
    "\n",
    "    def __init__(self, *, nexamples, lmbda):\n",
    "        super().__init__(lmbda=lmbda)\n",
    "        self.b = np.zeros(nexamples, dtype=int)\n",
    "        self.t = 1\n",
    "        self.support_vectors = None\n",
    "        self.labels_corresp_to_svs = None\n",
    "        self.sv_indices = None\n",
    "\n",
    "    def fit(self, *, X, y, kernel_matrix):\n",
    "        self.support_vectors = []\n",
    "        self.labels_corresp_to_svs = []\n",
    "        self.sv_indices = []\n",
    "        for j in range(len(X)):\n",
    "            self.t += 1\n",
    "            s = 0\n",
    "            for i in range(len(X)):\n",
    "                if y[j] == 0:\n",
    "                    y[j] = -1\n",
    "                s += self.b[i] * y[i] * kernel_matrix[i, j]\n",
    "            if y[j] / (self.lmbda * (self.t - 1)) * s < 1:\n",
    "                self.b[j] += 1\n",
    "\n",
    "        for i in range(len(self.b)):\n",
    "            if self.b[i] > 0:\n",
    "                self.support_vectors.append(X[i])\n",
    "                self.labels_corresp_to_svs.append(y[i])\n",
    "                self.sv_indices.append(self.b[i])\n",
    "\n",
    "        \n",
    "    def predict(self, *, X, kernel_matrix):\n",
    "        result = np.zeros(len(X), dtype=int)\n",
    "        for j in range(len(X)):\n",
    "            s = 0\n",
    "            for i in range(len(self.sv_indices)):\n",
    "                alpha = 1 / (self.lmbda * self.t) * self.sv_indices[i]\n",
    "                s += alpha * self.labels_corresp_to_svs[i] * kernel_matrix[i][j]\n",
    "            if s > 0:\n",
    "                result[j] = 1\n",
    "        return result\n",
    "    \n",
    "    def predict(self, *, X, kernel_matrix):\n",
    "        result = np.zeros(len(X), dtype=int)\n",
    "        for j in range(len(X)):\n",
    "            s = 0\n",
    "            for i in range(len(self.sv_indices)):\n",
    "                alpha = 1 / (self.lmbda * self.t) * self.sv_indices[i]\n",
    "                s += alpha * self.labels_corresp_to_svs[i] * kernel_matrix[i][j]\n",
    "            if s > 0:\n",
    "                result[j] = 1\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, kernel_type, ngram_length, train_epochs):\n",
    "    model = KernelPegasos(nexamples=len(X), lmbda=1e-3)\n",
    "    if kernel_type == 'ngram':\n",
    "        kernel = NgramKernel(ngram_length=ngram_length)\n",
    "    elif kernel_type == 'tfidf':\n",
    "        kernel = TFIDFKernel(X=X)\n",
    "    kernel_matrix = kernel.compute_kernel_matrix(X=X)\n",
    "\n",
    "    for epoch in range(train_epochs):\n",
    "        model.fit(X=X, y=y, kernel_matrix=kernel_matrix)\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model, X, kernel_type, ngram_length, train_epochs):\n",
    "    if kernel_type == 'ngram':\n",
    "        kernel = NgramKernel(ngram_length=ngram_length)\n",
    "    elif kernel_type == 'tfidf':\n",
    "        kernel = TFIDFKernel(X=model.support_vectors, X_prime=X)\n",
    "    kernel_matrix = kernel.compute_kernel_matrix(X=model.support_vectors, X_prime=X)\n",
    "\n",
    "    preds = model.predict(X=X, kernel_matrix=kernel_matrix)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715\n"
     ]
    }
   ],
   "source": [
    "# run our implementation of Kernel Pegasos SVM\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "a = []\n",
    "b = []\n",
    "for i in range(1000):\n",
    "    a.append(X[i])\n",
    "for i in range(1000):\n",
    "    a.append(X[len(X) - 1 - i])\n",
    "for i in range(1000):\n",
    "    b.append(y[i])\n",
    "for i in range(1000):\n",
    "    b.append(y[len(y) - 1 - i])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.20)\n",
    "\n",
    "model = train(X_train, y_train, \"ngram\", 3, 5)\n",
    "result = test(model, X_test, \"ngram\", 3, 5)\n",
    "same = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i] == y_test[i]:\n",
    "        same += 1\n",
    "\n",
    "print(same / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "edfcIbFJOSn6"
   ],
   "name": "ML_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
